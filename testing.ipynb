{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os,sys\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in germline2 .match file, convert to dict of valid entries (pairs with >=1 segment that's 5cM or more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchfile = '/data100t1/home/grahame/projects/compadre/unified-simulations/analysis-output/simulations/AMR/uniform3_size20_sim100/uniform3_size20_sim100/ss/ersa_input_final.txt'\n",
    "\n",
    "segment_dict = {}\n",
    "\n",
    "with open (matchfile, 'r') as f:\n",
    "    for line in f:\n",
    "        ls = line.split('\\t')\n",
    "        id1, id2 = ls[0], ls[1]\n",
    "        cmlen = round(float(ls[4]), 2)\n",
    "        key = f\"{id1}:{id2}\"\n",
    "        if cmlen >= 5.0:\n",
    "            if key not in segment_dict:\n",
    "                segment_dict[key] = (cmlen,)\n",
    "            else:\n",
    "                segment_dict[key] += (cmlen,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with segments as keys\n",
    "\n",
    "segment_dict_larger_1 = {}\n",
    "\n",
    "with open (matchfile, 'r') as f:\n",
    "    for line in f:\n",
    "        ls = line.split('\\t')\n",
    "        id1, id2, start, end, cmlen, chrom = ls[0], ls[1], int(ls[2]), int(ls[3]), round(float(ls[4]), 2), int(ls[5].strip())\n",
    "        key = f\"{chrom}:{start}:{end}:{cmlen}\"\n",
    "        value = f\"{id1}:{id2}\"\n",
    "        if cmlen >= 5.0:\n",
    "            if key not in segment_dict_larger_1:\n",
    "                segment_dict_larger_1[key] = (value,)\n",
    "            else:\n",
    "                segment_dict_larger_1[key] += (value,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with ids as keys\n",
    "\n",
    "segment_dict_larger_2 = {}\n",
    "\n",
    "with open (matchfile, 'r') as f:\n",
    "    for line in f:\n",
    "        ls = line.split('\\t')\n",
    "        id1, id2, start, end, cmlen, chrom = ls[0], ls[1], int(ls[2]), int(ls[3]), round(float(ls[4]), 2), int(ls[5].strip())\n",
    "        key = f\"{id1}:{id2}\"\n",
    "        value = f\"{chrom}:{start}:{end}:{cmlen}\"\n",
    "        if cmlen >= 5.0:\n",
    "            if key not in segment_dict_larger_2:\n",
    "                segment_dict_larger_2[key] = (value,)\n",
    "            else:\n",
    "                segment_dict_larger_2[key] += (value,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with ids as keys and tuple values \n",
    "\n",
    "segment_dict_larger_3 = {}\n",
    "\n",
    "with open (matchfile, 'r') as f:\n",
    "    for line in f:\n",
    "        ls = line.split('\\t')\n",
    "        id1, id2, start, end, cmlen, chrom = ls[0], ls[1], int(ls[2]), int(ls[3]), round(float(ls[4]), 2), int(ls[5].strip())\n",
    "        key = f\"{id1}:{id2}\"\n",
    "        value = (chrom, start, end, cmlen)\n",
    "        if cmlen >= 5.0:\n",
    "            if key not in segment_dict_larger_3:\n",
    "                segment_dict_larger_3[key] = (value,)\n",
    "            else:\n",
    "                segment_dict_larger_3[key] += (value,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have to figure out how to update the ERSA iteration to handle this data structure instead of the large file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary: 198702 bytes\n",
      "DataFrame: 795200 bytes (4x difference)\n"
     ]
    }
   ],
   "source": [
    "# In-memory footprint comparison with a dataframe\n",
    "\n",
    "df = pd.read_csv(matchfile, sep='\\t', header=None)\n",
    "\n",
    "dict_size = sys.getsizeof(segment_dict_larger_1) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in segment_dict_larger_1.items())\n",
    "df_size = df.memory_usage(deep=True).sum()\n",
    "difference_multiplier = int(df_size / dict_size)\n",
    "\n",
    "print(f\"Dictionary: {dict_size} bytes\\nDataFrame: {df_size} bytes ({difference_multiplier}x difference)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary (Segment keys): 198702 bytes\n",
      "Dictionary (ID pair keys, concat str values): 53914 bytes\n",
      "Dictionary (ID pair keys, tuple values): 53914 bytes\n"
     ]
    }
   ],
   "source": [
    "# Compare larger dict sizes \n",
    "\n",
    "dict_size1 = sys.getsizeof(segment_dict_larger_1) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in segment_dict_larger_1.items())\n",
    "dict_size2 = sys.getsizeof(segment_dict_larger_2) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in segment_dict_larger_2.items())\n",
    "dict_size3 = sys.getsizeof(segment_dict_larger_3) + sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in segment_dict_larger_3.items())\n",
    "\n",
    "print (f\"Dictionary (Segment keys): {dict_size1} bytes\\nDictionary (ID pair keys, concat str values): {dict_size2} bytes\\nDictionary (ID pair keys, tuple values): {dict_size3} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The smallest option is using id1:id2 as the key and a tuple of segment information as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1945629, 4586645, 6.2),)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_dict_larger_3['id2:id10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build 1KG superpopulation 'lists' to match HH's downloaded data, and validate by comparing EUR list to his ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_eur_id_file = '/data100t1/share/reference_data/1KG/GRCh38/EUR/1KG.EUR.ids'\n",
    "hh_eur_ids = []\n",
    "with open (hh_eur_id_file, 'r') as f:\n",
    "    for line in f:\n",
    "        hh_eur_ids.append(line.strip())\n",
    "\n",
    "hh_amr_id_file = '/data100t1/share/reference_data/1KG/GRCh38/AMR/1KG.AMR.ids'\n",
    "hh_amr_ids = []\n",
    "with open (hh_amr_id_file, 'r') as f:\n",
    "    for line in f:\n",
    "        hh_amr_ids.append(line.strip())\n",
    "\n",
    " \n",
    "id_info = pd.read_csv('/data100t1/home/grahame/projects/compadre/unified-simulations/data/igsr-1000 genomes 30x on grch38.tsv.tsv', sep='\\t')\n",
    "\n",
    "eur_ids = id_info[id_info['Superpopulation code'] == 'EUR']['Sample name'].to_list()\n",
    "amr_ids = id_info[id_info['Superpopulation code'] == 'AMR']['Sample name'].to_list()\n",
    "afr_ids = id_info[id_info['Superpopulation code'] == 'AFR']['Sample name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_test = pd.read_csv('/data100t1/home/grahame/projects/compadre/unified-simulations/data/other.tsv', sep='\\t', usecols=['SAMPLE_NAME', 'POPULATION'])\n",
    "\n",
    "other_ids = other_test['SAMPLE_NAME'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_eur = [x for x in other_ids if x in eur_ids]\n",
    "other_afr = [x for x in other_ids if x in afr_ids]\n",
    "other_amr = [x for x in other_ids if x in amr_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data100t1/share/reference_data/1KG/GRCh38/AFR/1KG.AFR.ids', 'w+') as outfile:\n",
    "    for x in other_afr:\n",
    "        outfile.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate all my unrelated subpop ids into a single list (for fixing the unrelated plink-binary set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpop_folder = '/data100t1/home/grahame/projects/compadre/primus/primus_old_1KG/lib/1KG_reference_data/1KG_unrelateds_by_subpop'\n",
    "\n",
    "subpop_files = [f'{subpop_folder}/{f.name}' for f in os.scandir(subpop_folder)]\n",
    "\n",
    "master_list = []\n",
    "\n",
    "for f in subpop_files:\n",
    "    df = pd.read_csv(f, sep='\\t')\n",
    "    ids = df['IID'].to_list()\n",
    "    master_list.extend(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{subpop_folder}/1KG_formatted_set_all', 'w+') as outfile:\n",
    "    for x in master_list:\n",
    "        outfile.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace duplicate FIDs in the all_unrelateds_NEW plink fileset with the corresponding subpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famfile = '/data100t1/home/grahame/projects/compadre/primus/primus_old_1KG/lib/1KG_reference_data/all_unrelateds_NEW.fam'\n",
    "famfile2 = '/data100t1/home/grahame/projects/compadre/primus/primus_old_1KG/lib/1KG_reference_data/all_unrelateds_NEW.fam2'\n",
    "\n",
    "subpop_folder = '/data100t1/home/grahame/projects/compadre/primus/primus_old_1KG/lib/1KG_reference_data/1KG_unrelateds_by_subpop'\n",
    "\n",
    "subpop_files = [f'{subpop_folder}/{f.name}' for f in os.scandir(subpop_folder) if not f.name.endswith('all')]\n",
    "\n",
    "subpop_dict = {}\n",
    "\n",
    "for f in subpop_files:\n",
    "    pop = f.split('_')[-1]\n",
    "    df = pd.read_csv(f, sep='\\t')\n",
    "    ids = df['IID'].to_list()\n",
    "    for x in ids:\n",
    "        subpop_dict[x] = pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (famfile, 'r') as infile, open (famfile2, 'w+') as outfile:\n",
    "    for line in infile:\n",
    "        ls = line.split(' ')\n",
    "        iid = ls[1]\n",
    "        popmatch = subpop_dict[iid]\n",
    "        outfile.write(f'{popmatch} {iid} 0 0 0 -9\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
